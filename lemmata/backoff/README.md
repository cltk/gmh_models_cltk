# Lemmata

All tokens are normalized according to [Referenzkorpus Mittelhochdeutsch] rules.
 
lemma_to_tokens.pickle: {lemma1: {token1_1, token1_2, ...], lemma2: {token2_1, token_2_2, ...}, ...}

DictLemmatizer: {token1: lemma1, token2: lemma2, ...} 
UnigramLemmatizer: list of sentences, where a sentence is [(token1, lemma1), (token2, lemma2, ...)]
